{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9IZRH6Bqhefbh8ceO4yrN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"OPCsS6isVXo-"},"outputs":[],"source":["Importing Libraries and Datasets\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, OneHotEncoder,PolynomialFeatures\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.svm import SVR\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import r2_score\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","from sklearn.ensemble import VotingRegressor\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","train = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/train.csv\")\n","test = pd.read_csv(\"/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/test.csv\")\n","Data Type Identification\n","The dataset contains the following columns with their respective data types:\n","\n","id: int64 (numerical, identifier)\n","airline: object (categorical)\n","flight: object (categorical, may have high cardinality)\n","source: object (categorical)\n","departure: object (categorical/time)\n","stops: object (categorical, can be converted to ordinal or integer)\n","arrival: object (categorical/time)\n","destination: object (categorical)\n","class: object (categorical)\n","duration: float64 (numerical, duration of flight)\n","days_left: float64 (numerical, derived feature)\n","price: int64 (numerical, target variable)\n","train.dtypes\n","id               int64\n","airline         object\n","flight          object\n","source          object\n","departure       object\n","stops           object\n","arrival         object\n","destination     object\n","class           object\n","duration       float64\n","days_left      float64\n","price            int64\n","dtype: object\n","Descriptive Statistics\n","train.describe()\n","id\tduration\tdays_left\tprice\n","count\t40000.00000\t36987.000000\t35562.000000\t40000.00000\n","mean\t19999.50000\t12.004088\t26.197936\t20801.49025\n","std\t11547.14972\t7.108063\t13.469232\t22729.14842\n","min\t0.00000\t0.830000\t1.000000\t1105.00000\n","25%\t9999.75000\t6.670000\t15.000000\t4687.00000\n","50%\t19999.50000\t11.080000\t26.000000\t7353.00000\n","75%\t29999.25000\t15.920000\t38.000000\t42521.00000\n","max\t39999.00000\t47.080000\t49.000000\t114704.00000\n","Missing Values Handling\n","Instead of dropping these rows, I decided to impute them to retain as much data as possible:\n","\n","Numerical columns (like duration, days_left) were imputed using the median.\n","Categorical columns (like airline, departure, stops) were imputed using the mode.\n","train.isnull().sum()\n","id                0\n","airline        4613\n","flight            0\n","source            0\n","departure      4792\n","stops          2319\n","arrival           0\n","destination       0\n","class             0\n","duration       3013\n","days_left      4438\n","price             0\n","dtype: int64\n","Duplicate Records Handling\n","train.duplicated().sum()\n","0\n","There are no duplicate rows in the dataset, so no action was required.\n","\n","Outlier Detection\n","I used boxplots to visually inspect outliers in the numerical columns of the dataset.\n","While some outliers are present, I decided not to remove them because the models I trained are primarily tree-based models (like Random Forest, XGBoost, etc.), which are inherently robust to outliers.\n","\n","So, retaining these values does not negatively affect model performance and helps preserve the natural distribution of the data.\n","\n","for i in (train, test):\n","    for col in i.columns:\n","        if i[col].dtype != 'object':\n","            Q1 = i[col].quantile(0.25)\n","            Q3 = i[col].quantile(0.75)\n","            IQR = Q3 - Q1\n","            lower_bound = Q1 - 1.5*IQR\n","            upper_bound = Q3 + 1.5*IQR\n","\n","            i[col].clip(lower=lower_bound, upper=upper_bound, inplace=True)\n","Remove id column\n","train = train.drop(columns = [\"id\"])\n","test = test.drop(columns = [\"id\"])\n","Data Preprocessing\n","X_train = train.drop(columns=[\"price\"])\n","Feature Encoding and Imputation\n","median_columns = [\"duration\", \"days_left\"]\n","ohe_columns = [\"airline\", \"source\", \"departure\", \"arrival\", \"destination\"]\n","ordinal_columns = [\"flight\", \"stops\", \"class\"]\n","column_transformer = ColumnTransformer(transformers=[\n","    ('median_col',  SimpleImputer(strategy='median'), median_columns),\n","\n","    ('ohe_col', Pipeline([\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n","    ]), ohe_columns),\n","\n","    ('ordinal_col', Pipeline([\n","        ('imputer', SimpleImputer(strategy='most_frequent')),\n","        ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n","    ]), ordinal_columns)\n","])\n","\n","X_train_transformed = column_transformer.fit_transform(X_train)\n","test_transformed = column_transformer.transform(test)\n","Feature Scaling\n","Since we are using tree-based models (like Random Forest, XGBoost), feature scaling is not necessary as they are not sensitive to feature magnitudes.\n","\n","Model Building\n","X = X_train_transformed\n","y = train[\"price\"]\n","Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1437)\n","Linear Regression\n","My Linear Regression model achieved an R² score of 0.90, which clearly crosses the required threshold of 0.80.\n","\n","regression_model = LinearRegression()\n","\n","regression_model.fit(X_train, y_train)\n","y_pred = regression_model.predict(X_test)\n","\n","regression_r2_score = r2_score(y_test, y_pred)\n","regression_r2_score\n","0.9077473123917048\n","Ridge Regression\n","This model also achieved an R² score of 0.90, which crosses the required threshold of 0.80, indicating strong predictive performance.\n","\n","ridge_model = Ridge()\n","\n","ridge_model.fit(X_train, y_train)\n","y_pred = ridge_model.predict(X_test)\n","\n","ridge_r2_score = r2_score(y_test, y_pred)\n","ridge_r2_score\n","0.9077462454303461\n","Lasso Regression\n","Lasso Regression was applied, and it achieved R² score of 0.90.\n","\n","lasso_model = Lasso()\n","\n","lasso_model.fit(X_train, y_train)\n","y_pred = lasso_model.predict(X_test)\n","\n","lasso_r2_score = r2_score(y_test, y_pred)\n","lasso_r2_score\n","0.9077376284473211\n","Polynomial Regression (Degree 2)\n","Polynomial Regression with degree 2 was applied to capture non-linear patterns in the data. The model performed well, achieving an R² score of 0.94\n","\n","polynomial_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n","polynomial_model.fit(X_train, y_train)\n","\n","y_pred = polynomial_model.predict(X_test)\n","poly_r2_score = r2_score(y_test, y_pred)\n","poly_r2_score\n","0.9446493130366147\n","K-Nearest Neighbors Regression\n","K-Nearest Neighbors (KNN) Regression was also applied. However, it performed relatively poorly compared to Linear, Lasso, and Ridge regressions. It achieved an R² score of 0.29\n","\n","KNN_model = KNeighborsRegressor()\n","KNN_model.fit(X_train, y_train)\n","\n","y_pred = KNN_model.predict(X_test)\n","knn_r2_score = r2_score(y_test, y_pred)\n","knn_r2_score\n","0.2906872653007082\n","Decision Tree Regression\n","Decision Tree Regression was applied and delivered impressive performance. It achieved an R² score of 0.96, outperforming Linear, Lasso, Ridge, and KNN models by effectively capturing complex patterns in the data.\n","\n","dt_model = DecisionTreeRegressor()\n","dt_model.fit(X_train, y_train)\n","\n","y_pred = dt_model.predict(X_test)\n","dt_r2_score = r2_score(y_test, y_pred)\n","dt_r2_score\n","0.9648737421307159\n","Random Forest Regression\n","The Random Forest model leveraged the power of ensemble learning by combining multiple decision trees. It captured complex patterns in the data effectively and delivered a high R² score of 0.97, showcasing its strong predictive capability compared to other models.\n","\n","rf_model = RandomForestRegressor()\n","rf_model.fit(X_train, y_train)\n","\n","y_pred = rf_model.predict(X_test)\n","rf_r2_score = r2_score(y_test, y_pred)\n","rf_r2_score\n","0.9793277095982745\n","Gradient Boosting Regression\n","Gradient Boosting Regression achieved an R² score of 0.95, slightly lower than Random Forest but still showing strong predictive performance.\n","\n","gboost_model = GradientBoostingRegressor()\n","gboost_model.fit(X, y)\n","\n","y_pred = gboost_model.predict(X_test)\n","gb_r2_score = r2_score(y_test, y_pred)\n","gb_r2_score\n","0.9552786617419292\n","XGBoost Regression\n","XGBoost Regression delivered the R² score of 0.97 . Its powerful gradient boosting mechanism nailed the patterns in the data with precision.\n","\n","xgb_model = XGBRegressor(random_state=42)\n","xgb_model.fit(X_train, y_train)\n","\n","y_pred = xgb_model.predict(X_test)\n","xgb_r2_score = r2_score(y_test, y_pred)\n","xgb_r2_score\n","0.976568126433574\n","LightGBM Regression\n","Last but not the least, LightGBM Regression achieved an impressive R² score of 0.97. Fast, efficient, and nearly on par with XGBoost — it proved to be a solid contender in the lineup.\n","\n","lgb_model = LGBMRegressor(random_state=42)\n","lgb_model.fit(X_train, y_train)\n","\n","y_pred = lgb_model.predict(X_test)\n","lgb_r2_score = r2_score(y_test, y_pred)\n","lgb_r2_score\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004871 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 614\n","[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 35\n","[LightGBM] [Info] Start training from score 20762.366187\n","0.9730827677362757\n","Hyperparameter Tuning & Model Selection\n","For hyperparameter tuning, I selected the top 3 performing models — Random Forest, XGBoost, and LightGBM — based on their high R² scores and overall performance.\n","\n","Model Selection\n","After comparing the R² scores of all three tuned models — Random Forest (0.9793), LightGBM (0.9789), and XGBoost (0.9812) — XGBoost emerged as the top performer. Therefore, it was selected as the final model for training and prediction.\n","\n","#RANDOM FOREST REGRESSION\n","rf_model = RandomForestRegressor(n_estimators=700,max_depth=24,min_samples_split=4,min_samples_leaf=1,max_features=None,random_state=42,n_jobs=-1)\n","rf_model.fit(X_train, y_train)\n","y_pred_val = rf_model.predict(X_test)\n","rf_r2_score = r2_score(y_test, y_pred_val)\n","\n","#XGBOOT REGRESSION\n","xgb_model = XGBRegressor(\n","    n_estimators=1431,\n","    max_depth=9,\n","    learning_rate=0.02682794029820691,\n","    subsample=0.9534150710821826,\n","    colsample_bytree=0.9042762617787571,\n","    reg_alpha=6.403811137420917,\n","    reg_lambda=2.0333285857846137,\n","    min_child_weight=2,\n","    gamma=3.2142404785425622,\n","    objective='reg:squarederror',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","xgb_model.fit(X_train, y_train)\n","y_pred_val = xgb_model.predict(X_test)\n","xgb_r2_score = r2_score(y_test, y_pred_val)\n","\n","#LIGHTGBM REGRESSION\n","lgb_model = LGBMRegressor(n_estimators=1000,max_depth=15,learning_rate=0.17236,subsample=0.7715,colsample_bytree=0.9248,reg_alpha=8.9567,reg_lambda=1.5674,min_child_weight=1,min_split_gain=0.4894,random_state=42,n_jobs=-1,verbose=-1)\n","lgb_model.fit(X_train, y_train)\n","y_pred_val = lgb_model.predict(X_test)\n","lgb_r2_score = r2_score(y_test, y_pred_val)\n","\n","print(f\"Tuned Random Forest R² Score: {rf_r2_score:.4f}\")\n","print(f\"Tuned XGBoost R² Score: {xgb_r2_score:.4f}\")\n","print(f\"Tuned LightGBM R² Score: {lgb_r2_score:.4f}\")\n","Tuned Random Forest R² Score: 0.9797\n","Tuned XGBoost R² Score: 0.9814\n","Tuned LightGBM R² Score: 0.9792\n","Final Model and Submission\n","Now, I am using 100% of the training data to fit the final model.\n","\n","xgb_model = XGBRegressor(\n","    n_estimators=1431,\n","    max_depth=9,\n","    learning_rate=0.02682794029820691,\n","    subsample=0.9534150710821826,\n","    colsample_bytree=0.9042762617787571,\n","    reg_alpha=6.403811137420917,\n","    reg_lambda=2.0333285857846137,\n","    min_child_weight=2,\n","    gamma=3.2142404785425622,\n","    objective='reg:squarederror',\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","xgb_model.fit(X, y)\n","\n","\n","y_pred = xgb_model.predict(test_transformed)\n","submission_df = pd.DataFrame({\n","    'id': range(10000),\n","    'price': y_pred\n","})\n","\n","\n","submission_df.to_csv('submission.csv', index=False)"]}]}